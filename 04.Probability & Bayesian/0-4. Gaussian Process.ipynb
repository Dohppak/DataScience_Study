{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process\n",
    "\n",
    "A random process $X(t)$ is a Gaussian process if for all $k ∈ N$ for all $t_{1},...,t_{k}$, a random vector formed by $X(1),...,X(t_{k})$ is jointly Gaussian.\n",
    "<br>\n",
    "\n",
    "랜덤 프로세스 $X(t)$가 가우시안 프로세스가 될려면, 모든 시점에 대해서, 시점에 대한 확률변수가 joint Gaussian이면 Gaussian Process라고 한다.\n",
    "<br>\n",
    "\n",
    "The joint density is completely specified by\n",
    "- Mean: m(t) = E(X(t)), where m(·) is known as a mean function.\n",
    "- Covariance: k(t,s) = cov(X(t),X(s)) =, where k(·,·) is known as a covariance function.\n",
    "- Notation: $X(t) ∼ GP(m(t),k(t,s))$\n",
    "\n",
    "Random Variable의 collection 이다. Density는 시간에 대한 평균과, 시간과 공분산이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Gaussian process란 무엇인가? \n",
    "\n",
    "\n",
    "\n",
    " 이번 포스팅의 주 목적인 Gaussian process (GP)에 대해서 생각해보자. GP는 그 이름에서도 알 수 있듯이 RP의 일종이다. 그리고, 그 관계가 몬가 가우시안 적일 것이다. 내가 지금 참고하고 있는 논문 [1]의 노테이션을 사용해서 설명해보겠다. \n",
    "\n",
    " \n",
    "\n",
    " 먼저 RP는 다음과 같이 정의한다. \n",
    "\n",
    "Probability space는 '$(\\Omega, F, P)$'이고, '$\\Omega$'는 sample space이다. 확률 변수의 모음으로 '$Z(x, w)$'로 나타난다. 이 때 '$x$'는 indexing variable이고, '$w$'는 Sample space의 event이다. \n",
    "\n",
    "\n",
    "\n",
    " 이러한 RP의 mean function과 covariance function은 다음과 같이 정의된다. \n",
    "\n",
    "$$ \\mu(x) = E(Z(x, w)) = \\int_{\\Omega} Z(x, w)dP(w) $$\n",
    "\n",
    "$$ C(x_i, x_j) = Cov(Z(x_i, w), Z(x_j, w)) = E((Z(x_i, w) - \\mu(x_i))(Z(x_j, w) - \\mu(x_j))) $$\n",
    "\n",
    "$$ = \\int_{\\Omega} (Z(x_i, w) - \\mu(x_i))(Z(x_j, w) - \\mu(x_j)) dP(w)$$\n",
    "\n",
    "\n",
    "\n",
    " 여기서 부터는 '$Z(x, w)$'에서 '$w$'의 dependency를 생략하겠다. 무슨 의민고 하니, 어떤 sample space로부터 한 event가 검출되고, 해당 event로부터 index '$x$'에 의해 어떤 한 값이 결정되는데, 이제부터는 '$Z$'가 index '$x$'에 soley dependent한다고 보는 것이다. 예를 들어 '$Z$'가 온도를 나타내고, '$x$'가 위치를 나타내면, 온도는 위치에 함수로 보겠다는 의미이다. 그리고 우리는 무한 개가 아닌 다음과 같은 유한 개의 set을 가정할 것이다. \n",
    "\n",
    "$$ \\{ Z(x_1), Z(x_2), ..., Z(x_n) \\} $$ \n",
    "\n",
    "물론 '$Z(x_i)$'는 '$Z(x), x \\in X$'의 collection이다. \n",
    "\n",
    "\n",
    "\n",
    " Gaussian process는 유한개의 finite dimensional distribution이 multivariate normal distribution을 따르는 RP를 의미한다. 다시 말해서 유한개의 확률 변수를 뽑아냈을 때 그것들이 모두 joint Gaussian distribution을 따른다. 그렇기 때문에 GP는 그것의 mean function과 covariance function 만 있으면 정의가 된다. 이는 마치 joint Gaussian distribution이 mean vector와 covariance matrix로 정의됨과 마찬가지이다. 또한 covariance function은 다음의 조건을 만족하는 positive definite function이어야 한다. \n",
    "\n",
    " $$ \\sum_{i=1}^{n}\\sum_{j=1}^{n} a_ia_jC(x_i, x_j) > 0 $$\n",
    "\n",
    "for every '$n$', every collection '$\\{ x_1, x_2, ... , x_n \\}$' and every vector '$a$'.\n",
    "\n",
    "\n",
    "\n",
    " 일반적인 GP는 mean-zero를 가정한다. 그리고 covariance function input 들의 차이(separation vector)의 함수이다. 다시 말해서 위에서 정의한 wide-sense stationary의 조건을 모두 만족한다. 추가로 만약 covariance가 distance metric 의 함수라면 해당 process는 isotropic이라고 한다. 이렇게 정의된 covariance function을 통해서 우리에게 익숙한 kernel (correlation) function을 유도할 수 있다. \n",
    "\n",
    "$$ R(x_i, x_j) = \\frac{C(x_i, x_j)}{\\sigma(x_i)\\sigma(x_j)} $$\n",
    "\n",
    "'$\\sigma^2(x_i) = C(x_i, x_i)$' 는 variance function 이다. \n",
    "\n",
    "\n",
    "\n",
    " wss를 만족하는 Gaussian process를 stationary Gaussian process라 한다. 물론 non-stationary GP도 있을 것이다. 이는 우리의 kernel function이 거리의 함수가 아님을 의미한다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Regression\n",
    "\n",
    "Frequentist view\n",
    "\n",
    "1. $f(x) = x^{T}w$\n",
    "2. $Y = f(x)+\\epsilon$\n",
    "3. $\\epsilon$ ~ $N(0,\\sigma^{2})$\n",
    "\n",
    "$$\n",
    "p(Y|X,w) = \\prod_{i=1}^{n}p(y_{i}|x_{i},w)\n",
    "$$\n",
    "\n",
    "#### w가 정해졌을때\n",
    "\n",
    "X,w가 주어졌을때, Y에 대한 joint distribtion은 i.i.d 컨디션에서, 모든 x에 대한 모든 y의 확률의 곱과 같다.\n",
    "- 입실론이 정규분포를 따르니까 x,y역시 정규분포를 따르게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE, MAP?\n",
    "\n",
    "- 항상 알고싶은 parameter를 prior 로 놓게 된다.\n",
    "- 단 이 prior를 가우시안 분포를 따른다고 가정하게 된다.\n",
    "$w$~$N(0,sigma_{p})$\n",
    "\n",
    "베이지안 목적은 항상 posterior 확률을 찾는거다!<br>\n",
    "Posterior? = 데이터가 있을대$(X,y)$,그때 $w$에 대한 확률이다.<br>\n",
    "\n",
    "$$\n",
    "p(w|y,X) = \\frac{p(y|X,w)p(w)}{p(y|X)}\n",
    "$$\n",
    "\n",
    "likelihood에 대해서 켤레 posterior가 나오게 됩니다.베이지안은 Posterior의 Mean을 구하고 싶다. 이때 이 posterior 확률이 극대화 되는 Posterior distribution을 MAP solution이라고 합니다.\n",
    "\n",
    "$$\n",
    "\\hat{W}_{MAP} = \\frac{1}{\\sigma_{n}^{2}}(\\frac{1}{\\sigma_{n}^{2}}XX^{T}+\\Sigma_{p}^{-1})^{-1}Xy\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{W}_{MLE} = (XX^{T})^{-1}Xy\n",
    "$$\n",
    "\n",
    "Prior 의 variance가 무한하면 (Prior에 대해서 아는게 없다) MAP가 MLE가 됩니다. 가우시안인데 variance가 무한하다? 아는게 없다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian formulation\n",
    "\n",
    "우리가 새로운 input $x^{*}$가 들어왔다가 해보자. 그렇다면 새로운 분포가 발생하게 된다. posterior가 변하게 된니다. 새로운 $x$가 들어왔을때 오는 posterior들의 mean을 계산해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Trick\n",
    "\n",
    "입력을 다른 공간에 보내서 그것을 linear regression 을 사용하게 된다. 커널은 feature space의 내적이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function space view\n",
    "\n",
    "이제 $f(x)$를 mean-zero Gaussian process이다. 데이터가 새롭게 들어온다면, Conditional Gaussian이 됩니다. \n",
    "\n",
    "- 우리가 관심있는 $f(x)$에 대해서 prior를 따른다고 가정하고, 모든 데이터가 가우시안에서 나왔다고 가정하게 됩니다.\n",
    "\n",
    "$$\n",
    "f_{*}=\\Sigma^{n}_{i=n}\\alpha_{i}k(x_{i},x_{*}) \\\\\n",
    "\\alpha = (K+\\sigma^{2}_{n}I)y \n",
    "$$\n",
    "mean function이 함수의 값이 커널에서 하나가 고정된 상태에서 base가 데이터 갯수만큼 있습니다. RKHS에 있는 것과 베이지안 mean function과 일치하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP\n",
    "- Pros : prinicled, probabilistic, predictive uncertatinty\n",
    "- Cons : Computational inefficiency O($n^{3}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPR\n",
    "\n",
    "kernel function : k(x,x')\n",
    "\n",
    "두 input 사이의 거리가 가까워지면, k 가 커집니다. 아니면 더 작아집니다. k의 의미는, 서로다른 x의 함수의 제곱의 평균입니다. 두 함수가 얼마나 유사한지에 대한 correlation입니다. 함수값 사이의 관계를 나타내는 겁니다. Corrlation이 작다 함수값이 급격하게 변한다. Corrleation이 크다. 함수가 매우 smooth 하다고 정의 가능합니다. 내가 관심있는 함수의 공간을 부드럽다고 가정하고 싶습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process latent variable model (GPLVM, PPCA)\n",
    "\n",
    "- Dimension reduction\n",
    "- Non-linear mapping\n",
    "\n",
    "SVG는 고차원 공간의 데이터를 잘 표현하는 축을 찾게 되는 것이다. 축의 갯수를 제한하면서 Dimension을 제거하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

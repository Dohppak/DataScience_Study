{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- Evaluation of test set : Trading & Test\n",
    "    - Check model\n",
    "- Generalized Evaluation : Trading & Validation & Test\n",
    "    - Check parameter & model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem of Evaluation on Small Data\n",
    "- The holdout method reserves a certain amount for testing and uses the remainder for training (Only random)\n",
    "- Stratified sample\n",
    "    - Advanced version of balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repected Holdout Method\n",
    "\n",
    "- Holdout estimate can be made more reliable by repeating the process with different subsamples\n",
    "    - In each iteration, a certain proportion is randomly selected for training\n",
    "    - The error rates on the different iterations are averaged to yield an overall error rate\n",
    "    - Still not optimum : the different test sets overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation\n",
    "- Avoids overlapping test sets (Maximum:10)\n",
    "    - First step : data is split into $K$ subsets of equal size\n",
    "    - Second step : each subsetin turn is usedfor testing and the remainder for training\n",
    "- Often the subsets are stratifiedbefore the cross-validation is performed\n",
    "- The error estimates are averaged to yield an overall error estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on Cross-Validation\n",
    "- Standard method for evaluation\n",
    "    - Stratified ten-fold cross validation\n",
    "    - Stratification reduces the estimate's variance\n",
    "    - Repeated stratified cross-validation\n",
    "- Leave one out cross validation\n",
    "    - It is a particular form of cross validation\n",
    "        - Set number of folds to number of traning instance\n",
    "        - n training instances, build classifier n times\n",
    "    - Makes best use of the data\n",
    "    - Involves no random subsampling\n",
    "    - Very computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criteria\n",
    "\n",
    "- Predictive accuracy : ability of the model to correctly predict the target of new or previously unseen data.\n",
    "- __Time & Memory__ : Computation costs involved in generating and using the model\n",
    "- Robustness :ability of the model to make correct predictions given nosiy data or data with missing values\n",
    "- __Scalability__ : ability to construct the model efficiently given large amount of data.\n",
    "<br/><br/>\n",
    "- Interpretability : The level of understanding and insight that is provided by the model\n",
    "- Simplicity : Decision tree size, rule compactness\n",
    "- Domain-dapendent quality indicators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Evaluation Prediction\n",
    "- Bias : Arithmeetic mean of the error\n",
    "- Mean Absolute Deviation (MAD)\n",
    "- __Mean Square Error (MSE)__\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- Root relative squared error (RRSE)\n",
    "<br/><br/>\n",
    "- In general, the lower the error measure, or the higher the R^2, r the better the forecasting model (Test & Trading) = y값과 예측 y 값 사이에서의 상관관계를 찾아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Evaluation Prediction\n",
    "- Two class : Yes, No\n",
    "- __Confusion matrix__ : Four different outcome : True positive(TP), true negative(TN), false postivie(FP), false negative(FN)\n",
    "<br/><br/>\n",
    "- Accuracy : Whole view\n",
    "- Sensitivity / Precision : Positive view\n",
    "- Specificity : Negative view\n",
    "- Recall : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC (Specificity & Sensitivity 사이의 균형을 잡아준다.)\n",
    "\n",
    "- AUC : Area under the Curve (base line 50%/80%)\n",
    "- 1-Specificity vs Sensitivity\n",
    "    - It show tradeoff between true positives and false postivies over noisy channel\n",
    "- Based on threshold : Sensitivity 와 Specificity 사이의 균형!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Measure\n",
    "- It can show tradeoff between precision and recall over noisy channel.\n",
    "- Recall and Precision : Text mining more offen focus on.\n",
    "- Only measure F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation, ACU, F1\n",
    "\n",
    "- Cross-validation \n",
    "    - Collect probabilities for instances in test folds\n",
    "    - 10sample, 10folder, 10reputation = 1000 (average)\n",
    "- Sort instances according to probabilities\n",
    "<br/><br/>\n",
    "- Generate an AUC or a F1 for each fold => avaerage\n",
    "- Generate an AUC or a F1 for each repetition =? average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

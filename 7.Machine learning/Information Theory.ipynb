{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory\n",
    "\n",
    "위 정보는 \"http://norman3.github.io/prml/docs/chapter01/6.html\" \"https://ratsgo.github.io/statistics/2017/09/22/information/\" \"https://datascienceschool.net/view-notebook/d3ecf5cc7027441c8509c0cae7fea088/\" 세 사이트를 참고했음을 밝힙니다\n",
    "\n",
    "### 정보란? : 학습에 있어서 필요한 놀랍의 정도 (degree of suprise)로 해석하면 된다.\n",
    "- 머신러닝에서는 해당 확률 분포의 틍성을 알아내거나 확률 분포간 유사성을 정량화 하는데 사용된다.\n",
    "- 잘 일어날 것 같지 않은 사건을 관찰하는 경우, 빈번하게 일어나는 사건보다 더 많은 정보를 취득했다고 고려하는 것이다\n",
    "- 따라서 항상 발생하는 일이라면, 사건 발생 후 얻는 정보의 양은 0이다.\n",
    "- 독립사건은 추가적인 정보량을 가진다\n",
    "- 이러한 속성은 확률에 종속적인 모양이라고 할 수 있다.\n",
    "<br/>\n",
    "\n",
    "### 정보의 양을 ${h(x)}$ 라고 정의한다면 정보는 결국 확률 함수의 조합으로 표현 될 것이다.\n",
    "$$ \n",
    "h(x)=-\\log_2p(x) \\qquad\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "- 정보의 양을 나타내는 함수 ${h(x)}$는 확률 함수 ${p(x)}$의 음의 로그 값을 볼 수 있다. (낮은확률에 큰 의미를 둔다)\n",
    "- 정보의 값은 0이상이므로, 0~1 사이의 확률 값에 로그를 붙이는 경우 앞에 음수가 붙어야 한다.\n",
    "- 사실 로그의 밑수(base)가 어떤 값이지는 상관 없으나 조건에 따라 적절한 것을 선택한다.\n",
    "- b =2 일떄는, ${h(x)}$의 기본단위가 비트라고 생각하면 된다.\n",
    "  \n",
    "### 엔트로피 (entropy)   \n",
    "- 이제 랜덤 변수 하나를 송신자가 수신자에게 전달한다고 가정해보자.\n",
    "- 이제 전송되는 데이터 양의 평균을 고려하면 어떻게 될까?\n",
    "- 전송되는 데이터 양의 평균을 엔트로피라고 정의한다.\n",
    "\n",
    "$$ H[x] = - \\sum_x p(x)\\log_2 p(x) \\qquad $$ \n",
    "\n",
    "- 랜덤변수 $X$에 대한 엔트로피  $\\lim_{p\\to0}p\\log_2p=0$ 이므로 $p(x)=0$ 이면 $p(x)log_2 p(x) = 0$ 이다\n",
    "\n",
    "- 엔트로피는 평균 정보량을 의미하며, p(x)인 분포에서 H(x) 함수의 기대값을 의미하게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시를 들어보자\n",
    "\n",
    "- 랜덤 변수 x 가 8개의 가능 값을 가지는 경우\n",
    "- 각각의 경우가 발현될 확률이 모두 동일하게 $1/8$인 경우 하나의 데이터 $x$를 전송하기 위해 필요한 비트의 수는 3이 된다\n",
    "\n",
    "$$\n",
    "H[x]=-8\\times\\frac{1}{8}\\log_2\\frac{1}{8}=3\\;bits\n",
    "$$\n",
    "\n",
    "- 현재 확률 분포가 Uniform 분포를 따르게 되므로 실제 데이터를 표현할 때 동일한 정보량을 가지는 형태로 표현하게 된다.\n",
    "- 앞서 언급한데로 Bit 단위로 정보량을 표현하면 임의의 데이터 한개를 데이터를 전송하기 위한 평균 bit 량은 3이 된다는 의미로 받아드려진다.\n",
    "\n",
    "$$\n",
    "H[x] = -\\frac{1}{2}\\log_2\\frac{1}{2} - \\frac{1}{4}\\log_2\\frac{1}{4}- \\frac{1}{8}\\log_2\\frac{1}{8} - \\frac{1}{16}\\log_2\\frac{1}{16} - \\frac{4}{64}\\log_2\\frac{1}{64} = 2\\;bits\n",
    "$$\n",
    "\n",
    "- 발현 빈도가 높은 데이터는 더 짧은 bit를 가지는 데이터로 표현하여 전체 전송량을 줄일 수 있게 된다.\n",
    "- 결국 Non-Uniform 분포의 엔트로피가 Uniform 엔트로피보다 낮다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 엔트로피 (entropy) 베이지안 접근\n",
    "- $Y=0$ 또는 $Y=1$ 인 두 가지 값을 가지는 확률 분포가 세 종류 있다고 하자.\n",
    "    - 확률분포 $Y_1 : P(Y=0)=0.5, P(Y=1)=0.5$\n",
    "    - 확률분포 $Y_2 : P(Y=0)=0.8, P(Y=1)=0.2$\n",
    "    - 확률분포 $Y_3 : P(Y=0)=1.0, P(Y=1)=0$\n",
    "\n",
    "만약 이 확률 분포가 베이지안 확률이라면, 확률분포 $Y_1$은 y값에 대해 아무것도 모르는 상태, $Y_3$는 0이라고 100% 확신하는 상태, $Y_2$는 y값이 0일것 같지만 불안한 상태를 나타낸다고 할수 있다.\n",
    "\n",
    "> #### 확률 분포들이 가지는 확신의 정도를 나타내는 수치를 엔트로피라고한다.\n",
    "\n",
    "확률분포들이 여러가지 값이 나올 확률이 대부분 비슷한 경우 엔트로피가 높아진다. 반대로 특정 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피가 작아진다.\n",
    "> #### 여러가지로 고루 분산되어 있을 수 있으면 엔트로피가 높고, 특정 하나의 상태로 몰려있으면 엔트로피가 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엔트로피의 성질\n",
    "\n",
    "- 엔트로피 최소값\n",
    "만약 확률 분포가 결정론적이면, 즉 특정한 하나의 값이 나올 확률이 1이고, 나머지 값은 나올 수 없다면 엔트로피는 0이며 이 값이 가장 작은 엔트로피이다.\n",
    "\n",
    "- 엔트로피 최대값\n",
    "엔트로피 최대값은 이산 확률변수의 클래스의 개수에 따라 달라진다. 만약 이산 확률 변수가 가질 수 있는 클래스가 $2^{K}$개 일때, 이산 확률 변수가 가질수 있는 엔트로피의 최댓값은 각 클래스가 모두 같ㅌ은 확률을 가질 떄이다.\n",
    "$$\n",
    "H = -\\frac{2^K}{2^K}\\log_2\\dfrac{1}{2^K} = K\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엔트로피와 정보량\n",
    "\n",
    "__엔트로피는 확률변수가 담을 수 있는 정보량을 표시한다__. 확률변수가 담을 수 있는 정보량이란 확률변수의 표본값을 관측해서 얻을 수 있는 정보의 종류를 말한다. \n",
    "\n",
    "- 엔트로피 0 : 결정론적, 확률변수의 표본값이 동일하다. 즉 관측해도 얻는 정보가 없다.\n",
    "- 엔트로피 크다 : 확률 변수의 표본값이 가질 수 있는 실질적인 경우의 수가 증가, 즉 표본값이 가져다 주는 정보량이 많다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조건부 엔트로피\n",
    "\n",
    "상관관계 있는 두 확률 변수 $X,Y$에 대해 $X$의 값을 안다면 $Y$의 확률 변수가 가질 수 있는 정보의 양을 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크로스 엔트로피\n",
    "\n",
    "> 분류 문제의 목표값 분포와 예측값 분포를 비교하는데 사용된다\n",
    "\n",
    "두 확률 분포 $p(y),q(y)$의 크로스 엔트로피는 다음과 같이 정의한다.\n",
    "$$\n",
    "-\\sum_{k=1}^K p(y_k) \\log_2 q(y_k)\n",
    "$$\n",
    "\n",
    "#### 예측 값과 실제 값이 같다면 - 다르면 다를수록 크로스 엔트로피는 증가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kullback-Leibler divergence (쿨백-라이블러 발산)\n",
    "\n",
    "> 크로스 엔트로피에서 대상이 되는 분포의 엔트로피를 뺸 값이다. 상대 엔트로피라고도 한다. 이것은 항상 양수이며 분포가 완벽하게 강한 경우 0이다\n",
    "\n",
    "Relative Entropy\n",
    "$$\n",
    "KL(p(y) || q(y)) \n",
    "= -\\int p(y) \\log_2 q(y) \\, dy - \\left( -\\int p(y) \\log_2 p(y) \\, dy \\right) \n",
    "= \\int p(y) \\log_2 \\left(\\dfrac{p(y)}{q(y)}\\right) dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

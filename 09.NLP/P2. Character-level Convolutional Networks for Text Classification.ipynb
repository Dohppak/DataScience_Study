{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 2015 | 풀잎스쿨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "1. Text classification에서 Character-level convolutional networks(ConvNets)를 BoW, n-gram, TFIDF variants, word-based ConvNets, RNN과 비교한다.\n",
    "2. 여러 데이터 셋에서 SOTA?\n",
    "3. 기존방식이랑 비교해 보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text classification는 free-text documents에 predefined category를 부여하는 NLP영역의 토픽입니다. Text Classification연구는 ML classifier로 넣어 줄 최적의 feature를 정하는 것에서 시작되었다. \n",
    "<br>\n",
    "본 논문에서는, text를 raw signal로 받기 위해 character level model을 사용했고, 1-D convolution 을 사용하다. 여태까지 CNN 모델들은 input의 단위는 단어였다(embedded word vector)였다. 하지만 이번 논문에서는 character 단위의 CNN을 사용해서 문서 분류 문제를 해결하고자 했다. \n",
    "<br>\n",
    "데이터 셋이 매우 큰 경우, ConvNet은 단어에 대한 정보(통사론 단위의 의미론적인 정보를 포함한다.)를 필요하지 않는다. 이렇게 character 단위로 만들어진 모델은 조금 수정으로도 여러 언어에 적용 가능하고, 오타나 이모티콘 같은 일반적인 단어와 마찬가지로 잘 학습이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Convolution Networks\n",
    "\n",
    "__Key-Module__\n",
    "\n",
    "가장 핵심적인 부분은 temporal convlutional 모듈이다. 이것은 1-D 컨볼류션으로, discrete 한 input과 kernel 을 가지게 된다. 그러면 Convlution을 다음 과 같은 함수로 표현이 가능하다. \n",
    "$$\n",
    "g(x) = input \\\\\n",
    "f(x) = kernel \\\\\n",
    "h(y) = output \\\\\n",
    "h(y)\\in[1,\\lfloor(l-k)/d\\rfloor+1] \\\\\n",
    "h(y)=\\sum^k_{x=1}f(x)\\cdot g(y\\cdot d-x+c)\n",
    "$$\n",
    "위에서 사용된 상수 $c$는 c=k-d+1 로 offset 상수이다. 또한 output $h_{j}(y)$는 모든 $i$에 대해서 input과 kernel을 convolution 연산하면서 얻어진다.\n",
    "<br>\n",
    "\n",
    "Deep-Learning 과정에서 6 layer의 깊이에서, max-pooling을 사용한다. \n",
    "$$\n",
    "h(y)=\\max^k_{x=1}g(y\\cdot d-x+c)\n",
    "$$\n",
    "그리고 non-linearity를 위한 함수로 ReLU 함수를 사용하고, 128 크기의 minibatch로 Stochastic gradient descent를 사용했으며 모멘텀을 사용해 updata를 진행했다. 모멘텀 사용 시 모멘텀 상수는 0.9로 하였고, 학습률은 0.01에서 시작해서 3epoch 마다 절받으로 줄였다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character quantization\n",
    "\n",
    "모델에서 인코딩된 글자들의 sequence를 input값으로 받았다. 여기서 인코딩은 m개의 알파벳에 대한 one-hot 인코딩방식이다. input은 m-Dimension이 된다. 만약 알파벳에 들어가지 않는 문자에 대해서는 0배거로 만든다. 특정 길이 까지만 input으로 받는다. 길이를 넘어가는 값에 대해서는 무시한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design\n",
    "\n",
    "2개의 ConvNet을 디자인 했으면, 하나는 많은 feature를 가지는 Conv고 하나는 적은 feature인 Conv다. 전체 모델은 9개 레이어로 구성되었으며, 6개를 Conv-layer 그리고 3개는 fully-connected layer로 구성되었다.\n",
    "<br>\n",
    "\n",
    "1. 하나의 문자당 70 Dimension(소문자)의 vector로 encoding이 진행된다. embedding 이 아니다.\n",
    "    - 1014 개의 char를 입력 sequence로 받는다. 1014개로 문장의 글자가 끊기는건가? \n",
    "    - batch size, 70, 1014, 계행 문자도 같이 들어간다. 그냥 하나씩 들어간다. 짧은 문장은 패딩한다. \n",
    "    - 1014은 task에 성능을 찾아볼 수도 있다.\n",
    "    - 리뷰의 길이의 히스토그램 중, 대표값을 사용한다.\n",
    "    \n",
    "2. input 70 by 1014로 구성한다. 의 단어 백터가 쌓인 매트릭스가 됩니다.\n",
    "3. 1-d convolution 을 취함, 각 conv는 6개의 Conv + relu + Maxpool 의 형태입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation based Comparison\n",
    "\n",
    "비교를 위한 모델은 전통적인 NLP 방식과 Deep Learning방식 두가지를 모두 사용해서 비교했다.\n",
    "<br>\n",
    "\n",
    "__Traditional Method__\n",
    "- Bag of words and its TFIDF\n",
    "- Bag of ngrams and its TFIDF\n",
    "- Bag of means on word embedding\n",
    "    - KNN\n",
    "<br>\n",
    "\n",
    "__Deep Learning Methods__\n",
    "- Word based ConvNets\n",
    "- LSTM\n",
    "    - 마지막 스테이트를 합친다?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concolusion\n",
    "\n",
    "- 글자 단위의 Convolutional Network도 문서 분류에서 높은 성능을 보인다.\n",
    "- 작은 데이터셋에서는 전통적인 NLP방식이 DL방식보다 더 높은 성능을 보인다.\n",
    "- ConvNet은 사용자가 만든 데이터에서 좋다.(오타를 잘 잡는다)\n",
    "- Alphabet의 선택에 따라 성능이 많이 달라진다.\n",
    "- Bag-of-means 모델은 안좋다.\n",
    "- 모든 데이터셋에 있어 최적의 모델은 없다.( 많은 실험을 통해 데이터셋에 가장 적합한 모델을 찾아야 한다)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글같은경우는 11000개의 charact인데, unicode index를 기준으로 Convolution을 하는게 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "- SoftMax function : The equivalent of the sigmoid activation function, But when 3 or more classes\n",
    "- 소프트멕스 함수 : 다양한 class 에 대해서 sigmoid와 같은 활성화 함수 기능을 합니다.\n",
    "    - Expoential function : 오직 positive values 만 returns 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    # 지수함수로 보낸다.\n",
    "    expL = np.exp(L)\n",
    "    # 지수함수에 있는 애들을 합친다.\n",
    "    sumExpL = sum(expL)\n",
    "    # list 자료형\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax2(L):\n",
    "    expL = np.exp(L)\n",
    "    return np.divide(expL, expL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax2([5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "- Input data 가 categorical data 라면?  1,0 으로 bool 값을 표현하는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "- Maximize probability and Minimize cost 가 같은 기능을 할까? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy (Error Function)\n",
    "\n",
    "- logarithm : 로그함수는 곱셈을 덧셈으로 바꾸는 성질이 있다. (summation 만 하면 충분하다!)\n",
    "- 나쁜 모델에게 높은 Cross Entropy를 좋은 모델에게는 낮은 Cross Entropy를 부여하게 한다.\n",
    "- 정확도가 높은것은 확률이 1에 가까워진다. 그 말은 정확도가 높을 수록 1에 가까워지며 이는 log에 들어가면 0에 가까워진다. \n",
    "\n",
    "#### 정확도가 높은 확률의 최대화가 Cross-Entropy의 최소화와 비슷한 의미를 가지게 만들 수 있다.\n",
    "\n",
    "$$\n",
    "-\\log{P(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y,P):\n",
    "    # Y = label, P = Probability\n",
    "    Y = np.float_(Y) #데이터 도메인\n",
    "    P = np.float_(P) #데이터 도메인\n",
    "    # - sum을 이용해서 더하고, logprobability에 Y를 곱한다\n",
    "    return -np.sum(Y * np.log(P) + (1-Y) * np.log(1-P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.828313737302301"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1,0,1,1],[0.4,0.6,0.1,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClass Cross-Entropy \n",
    "\n",
    "- Y-label의 class가 2개가 아니라 그 이상일때는 어떻게 해야할까?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

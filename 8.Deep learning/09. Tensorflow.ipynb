{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로 1.0의 경우 선형대수 심볼 컴파일러입니다. (Symbolic Linear Algebra Complier)\n",
    "\n",
    "1. 심볼 변수 정의\n",
    "2. 심볼 관계 정의\n",
    "3. Session 정의\n",
    "4. Session 사용\n",
    "\n",
    "텐서 플로우에서는 데이터는 integers,floats,strings으로 저장되지 않습니다. 데이터들은 tensor라는 객체에 encapsulated 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프와 세션\n",
    "\n",
    "텐서플로우는 모든 연산을 CPU 혹은 GPU에서 처리한다고 가정한다. 따라서 변수의 값을 할당하는 간단한 연산부터 복잡한 연산까지 프로그램이 돌아가는 큼퓨터 자체 내에서 이루어지는 것은 없다. 그렇다면 Tensorflow에서는 어떤것을 다룰 수 있을까? 바로 Tensor이다. 그리고 텐서자체와 텐서의 계산괒어은 모두 Graph라고 부르는 객체 내에 저장된다. 그리고 그래프를 계산하려면 외부 컴퓨터에 이 그래프 전체를 전달하고 그 결과값을 받는 것이 바로 Session이다.\n",
    "<br>\n",
    "\n",
    "따라서 모든 계산은 해당하는 그래프를 세션 객체에 전달하여 원격 실행 한 후에야 값을 볼 수 있다. 변수에 상수를 할당하는 연산조차 마찬가지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프\n",
    "\n",
    "텐서를 생성하면 그래프의 노드가 된다. 노드는 아무것도 지정하지 않으면 자동적으로 기본 그래프에 할당된다. 현재 그래프에 대한 정보를 얻는 방식은 tf.get_default_graph() 함수를 통해 진행된다. 만약 여러개의 그래프를 다루고 싶을 땐, 직접 그래프를 생성하고 지정하여 사용해야한다. tf.Graph() 함수를 이용해 그래프를 생성하고, with문 내에서 그래프를 구성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_default_graph at 0x111e39158>\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_default_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    var = tf.Variable(initial_value=1)\n",
    "\n",
    "# with문 내에서 선언한 변수는 직접 생성한 그래프의 노드와 같다는 것을 알수 있다.\n",
    "print(var.graph is graph)\n",
    "print(var.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Session()\n",
    "\n",
    "텐서플로우의 api는 computational graph의 형태로 설계가 되었습니다. 이것은 mathematical process를 시각화시킵니다. Session은 환경속에 있는 모든 그래프들을 작동하게 합니다. Session은 GPU나 CPU를 작동시킨다고 생각하시면 됩니다.\n",
    "\n",
    "- 세션 생성 : Session 객체 생성\n",
    "- 세션 사용 : run 그래프의 값을 계산하고 결과를 반환한다.\n",
    "- 세션 종료 : close 보통 with 문으로 대체한다.\n",
    "    - with 문 블럭을 나갈때 자동으로 close 메서드가 호출된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "hello_constant = tf.constant('Hello World!')\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 자료형의 종류\n",
    "\n",
    "Tensorflow에서 데이터의 source는 3가지가 있다.\n",
    "- 상수형(constant): 미리 주어진 고정값\n",
    "- 변수형(Variable): 계속 변화하는 값\n",
    "- 플레이스홀더(Placeholder) : 고정된 크기이지만 값이 미리 주어지지 않는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.constant()\n",
    "\n",
    "여기서 hello constant는 0-차원의 string tensor입니다. 그러나 tensor는 다양한 사이즈로 변화가 가능합니다.\n",
    "<br>\n",
    "constant tensor는 tensor의 값이 바뀌지 않기 때문에 constant라는 이름이 붙게 됩니다.\n",
    "\n",
    "- tf.zeros(shape)\n",
    "- tf.ones(shape)\n",
    "- tf.fill(shape, value)\n",
    "- tf.zeros_like(tensor)\n",
    "- tf.ones_like(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "filled_tsr = tf.fill([2,3],2)\n",
    "with tf.Session().as_default():\n",
    "    print(filled_tsr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant(1234)\n",
    "B = tf.constant([123,456,789]) \n",
    "C = tf.constant([ [123,456,789], [222,333,444]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive session and eval()\n",
    "\n",
    "간단한 작업 조차 세션을 통해서 해야하기때문에, interactivesession을 생성한 후에는 텐서들은 eval() 매서드를 통해서 세션을 지정하지 않더라도 자동으로 세션으 호출되어 텐서의 값이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(tf.zeros_like(B).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones_like(C).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "열(sequence)로 구성된 상수형 텐서를 만들떄는 range와 linspace등의 메서드를 사용할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(tf.range(5).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.5555556 1.1111112 1.6666667 2.2222223 2.777778  3.3333335\n",
      " 3.888889  4.4444447 5.       ]\n"
     ]
    }
   ],
   "source": [
    "print(tf.linspace(0.0,5.0,10).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to make random\n",
    "\n",
    "- random_uniform(shape, minval=0, maxval=None, seed=None)\n",
    "- random_noraml\n",
    "- truncated_normal\n",
    "- random_shuffle\n",
    "- random_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10086262 0.9701668  0.8487642 ]\n",
      " [0.04828131 0.04852307 0.77747464]]\n"
     ]
    }
   ],
   "source": [
    "print(tf.random_uniform((2,3), seed=0).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "np_array = np.arange(5)\n",
    "print(tf.convert_to_tensor(np_array).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.variables\n",
    "\n",
    "뉴럴넷을 학습시킬려면 일반적으로 weights와 variable들을 가지고 학습을 시키게 됩니다. 하지만 이들은 변수입니다. 이러한 변하는 값을들 저장하는 방식을 알아봅시다.\n",
    "<br>\n",
    "\n",
    "tf.Variable은 tensor를 생성한다. 이는 일반적인 동적인 파이썬 변수와 유사하다. 이 텐서는 session에 상태를 가지고 저장된다. 따라서 반드시 init의 상태를 지정해 주어야한다.\n",
    "<br>\n",
    "\n",
    "변수형 텐서는 inital_value 라는 속성에 초기화할 값을 저장해 두었다가, initializer를 이용하여 값을 변수에 할당한다. initial_value 속성을 확인해보면 우리가 초기값으로 입력했던 텐서의 정보가 담겨있다.\n",
    "<br>\n",
    "\n",
    "그렇다면 tf.global_variables_initializer()함수는 무엇을 할까?\n",
    "함수의 input에 변수의 초기화 연산자가 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_8:0' shape=() dtype=int32_ref>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.global_variables_initalizer()\n",
    "\n",
    "변수를 생성할 때 입력한 초기값은 사실 변수가 세션에서 초기화할 값을 지정하는 것으로, 실제로 해당 변수의 값으로 해당 된 것이 아니다. 따라서 지정한 값을 변수의 값으로 할당하는 과정이 필요한데 이것이 초기화 연산자이다.\n",
    "\n",
    "<br>\n",
    "global variables initalizer 는 텐서플로우 변수들을 그래프로부터 모두 initalize시킨다. initalizing하는 weights 는 normal distribution으로부터 랜덤하게 부여된다. Randomizing 된 weights는 학습을 통해서 개선되게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "interactive_sess.run(tf.global_variables_initializer())\n",
    "print(x.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### op.node_def\n",
    "\n",
    "변수를 생성할때 입력한 값과 데이터 타입에 대한 정보를 보여준다. 이 노드에 대한 정보 중에는 우리가 실제 지정한 값에 대한 정보는 들어있지 않다. 변수는 특정한 크기를 가진 데이터가 들어올 공간만을 마련해 둔 것이다. 초기화를 할당하는 작업 없이 그래프를 연산하고자 하면 그래프 노드에는 실제 값이 들어있지 않기 때문에 에러가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Variable\"\n",
      "op: \"VariableV2\"\n",
      "attr {\n",
      "  key: \"container\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shared_name\"\n",
      "  value {\n",
      "    s: \"\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(var.op.node_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.placeholder\n",
    "\n",
    "플레이스홀더 Tensor를 사용하면 상수형 Tensor와 같은 역할을 하지만, 미리 그 값을 넣어놓는 것이 아니라, session 을 사용한 그래프의 연산 중에 텐서 값을 설정한다. \n",
    "<br>\n",
    "\n",
    "어디에 쓰는가?\n",
    "신경망 학습의 경우 대부분 batch단위의 학습이 이루어지기 때문에 학습용 데이터는 플레이스 홀더에 넣어준다. 플레이스홀더는 데이터 타입이나 shape을 인수로 설정하여 생성하고, session을 실행할때 __feed_dict__ 을 인수로 채울 데이터를 설정하면 된다. Shape의 인수를 설정할 때, 데이터에 대한 것을 None으로 설정하면 갯수가 바뀌는 데이터에 대해서도 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.int32, shape=(None,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 3.]]\n",
      "[[6. 6. 6.]]\n",
      "[[9. 9. 9.]]\n",
      "[[12. 12. 12.]]\n",
      "[[15. 15. 15.]]\n",
      "[[18. 18. 18.]]\n",
      "[[21. 21. 21.]]\n",
      "[[24. 24. 24.]]\n",
      "[[27. 27. 27.]]\n",
      "[[30. 30. 30.]]\n"
     ]
    }
   ],
   "source": [
    "real_data = np.asarray([[i+_ for i in range(3)] for _ in range(10)])\n",
    "\n",
    "w = tf.ones((3,3), dtype=tf.float32)\n",
    "data = tf.placeholder(dtype=tf.float32, shape=(1, 3))\n",
    "\n",
    "result =tf.matmul(data,w)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for r in real_data:\n",
    "        result_ = sess.run(result, feed_dict = {data:r.reshape(1,3)})\n",
    "        print(result_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.truncated_normal()\n",
    "\n",
    "Weight를 초기화 시킬때, 다른 weight값이 overwhelming되는것을 막아준다. 역시 normal distribution에서 값을 지정한다.\n",
    "<br>\n",
    "\n",
    "variation을 2standard deviations으로 제한하여 뽑기때문에 그렇게 큰 값이 나오지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features,n_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_1:0' shape=(120, 5) dtype=float32_ref>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.zeros()\n",
    "\n",
    "일반적으로 bais 를 0 으로 놓고 시작하는 경우가 많다 이떄 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.softmax\n",
    "우리는 softmax function을 주로 input들을 logits or logit scores로 만들기 위해서 사용했다. 이는 input들을 0,1사이로 nomalize시키고 summation했을때 1이 되어 확률론적 접근을 가능하게 만들어 주었다. 이것의 의미는 softmax function은 categorical probability distribution으로 만들어 준다는 소리이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0,1.0,0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        ouput = sess.run(softmax, feed_dict={logits:logit_data})\n",
    "    return ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6590012 , 0.24243298, 0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf를 이용한 연산\n",
    "\n",
    "- tf.reduce_all : 'logical and' 연산\n",
    "- tf.reduce_any : 'logiocal or' 연산\n",
    "- tf.reduce_logsumexp : 'log(sum(exp(element' 연산\n",
    "- tf.reduce_max : max연산\n",
    "- tf.reduce_mean : 평균연산\n",
    "- tf.reduce_min : 최소값 연산\n",
    "- tf.reduce_prod : product연산\n",
    "- tf.reduce_sum : sum 연산\n",
    "- tf.gradient : 미분 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5.0)\n",
    "f = x**2\n",
    "fx = tf.gradients(f,[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "[10.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(f))\n",
    "    print(sess.run(fx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy in tensorflow\n",
    "\n",
    "\n",
    "Cross-entropy가 높다는 의미는 낮은 확률로 event가 발생한다인 것이다.\n",
    "정확도가 높은것은 확률이 1에 가까워진다. 그 말은 정확도가 높을 수록 1에 가까워지며 이는 log에 들어가면 0에 가까워진다.\n",
    "<br>\n",
    "\n",
    "softmax function으로서(3개 이상의 class를 구별할때 썼다!), Tensorflow는 cross entropy 연산 또한 지원한다. cross entropy를 loss-function으로 사용하고자 한다면, $\\hat{y}$와 $y$ 사이의 entropy function을 지정하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cn](./img/09.tf1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.reduce_sum([1,2,3,4,5])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Log:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =tf.log(100.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code of Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667497\n"
     ]
    }
   ],
   "source": [
    "softmax_data = [0.7,0.2,0.1]\n",
    "one_hot_data = [1.0,0.0,0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "\n",
    "미니배치는 데이터셋을 subset으로 나누어서 학습을 시키는 방식이다. 데이터셋이 한번에 학습에 들어가게 되면 시간과 메모리 사용이 매우 커진다. 이것을 방지하기 위해 데이터 셋을 나누어서 진행하게 된다.\n",
    "<br>\n",
    "\n",
    "미니 배치와 SGD를 함께 사용한다면 각 epoch의 시작부분에서 랜덤하게 데이터를 shuffle하는 효과가 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    assert len(features) == len(labels)\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['F11', 'F12', 'F13', 'F14'], ['F21', 'F22', 'F23', 'F24']],\n",
       "  [['L11', 'L12'], ['L21', 'L22']]],\n",
       " [[['F31', 'F32', 'F33', 'F34'], ['F41', 'F42', 'F43', 'F44']],\n",
       "  [['L31', 'L32'], ['L41', 'L42']]]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches(2, example_features, example_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "\n",
    "Epoch는 전체 데이터셋에 대하여 forward와 backward pass가 반복되는것을 의미한다. Epoch의 횟수는 데이터수의 증가없이도 모델의 accuracy를 높이는데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Neural Networks\n",
    "\n",
    "만약 먹티 레이어 뉴럴넷을 설계하고자 한다면, 히든레이어를 추가하여 좀더 complex한 모델을 제작할 수 있다. 또한 non-linear한 activation 함수를 사용한다면, 모델을 non-linear하게 제작이 가능하다.\n",
    "<br>\n",
    "\n",
    "우리는 첫번째로 ReLU를 hieen layer로 사용해볼 생각이다. ReLU는 non-linear function이자 rectified linear unit이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![cn](./img/09.tf2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bias\n",
    "\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hideen_layer = tf.nn.relu(hidden_layer)\n",
    "logit = tf.add(tf.matmul(hidden_layer,weights[1]),biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11      8.440001]\n",
      " [-5.11     -8.440001]\n",
      " [24.010002 38.239998]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process in Deep Neural Network\n",
    "\n",
    "- Setting learning Parameters\n",
    "- Setting Hidden Layer Parameters\n",
    "- Weights and Biases\n",
    "- Input\n",
    "- Multi-layer\n",
    "- Optimizer\n",
    "- Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the first layer to a Dense() layer with an output width of 8 nodes and the input_dim set to the size of the training samples (in this case 2).\n",
    "\n",
    "- Add a tanh activation function.\n",
    "- Set the output layer width to 1, since the output has only two classes. (We can use 0 for one class and 1 for the other)\n",
    "- Use a sigmoid activation function after the output layer.\n",
    "- Run the model for 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras.models.Sequential class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like compile(), fit(), and evaluate() that are used to train and run the model. We'll cover these functions soon, but first let's start looking at the layers of the model.<br>\n",
    "\n",
    "Layers\n",
    "The Keras Layer class provides a common interface for a variety of standard neural network layers. There are fully connected layers, max pool layers, activation layers, and more. You can add a layer to a model using the model's add() method. For example, a simple model with a single hidden layer might look like this:<br>\n",
    "\n",
    "Keras requires the input shape to be specified in the first layer, but it will automatically infer the shape of all other layers. This means you only have to explicitly set the input dimensions for the first layer.<br>\n",
    "\n",
    "The first (hidden) layer from above, model.add(Dense(32, input_dim=X.shape[1])), creates 32 nodes which each expect to receive 2-element vectors as inputs. Each layer takes the outputs from the previous layer as inputs and pipes through to the next layer. This chain of passing output to the next layer continues until the last layer, which is the output of the model. We can see that the output has dimension 1.<br>\n",
    "\n",
    "The activation \"layers\" in Keras are equivalent to specifying an activation function in the Dense layers (e.g., model.add(Dense(128)); model.add(Activation('softmax')) is computationally equivalent to model.add(Dense(128, activation=\"softmax\")))), but it is common to explicitly separate the activation layers because it allows direct access to the outputs of each layer before the activation is applied (which is useful in some model architectures).<br>\n",
    "\n",
    "Once we have our model built, we need to compile it before it can be run. Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data. We'll specify the loss function to be categorical_crossentropy which can be used when there are only two classes, and specify adam as the optimizer (which is a reasonable default when speed is a priority). And finally, we can specify what metrics we want to evaluate the model with. Here we'll use accuracy.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungheondoh/anaconda3/envs/python3.6/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 21ms/step\n",
      "\n",
      "Accuracy:  0.75\n",
      "\n",
      "Predictions:\n",
      "[[5.3688812e-01 4.0410689e-04]\n",
      " [1.0012478e-12 1.3889283e-09]\n",
      " [1.0000000e+00 9.9837708e-01]\n",
      " [5.3451467e-01 4.3506885e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our data\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "\n",
    "# Initial Setup for Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "# One-hot encoding the output\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "# Building the model\n",
    "xor = Sequential()\n",
    "xor.add(Dense(256, input_dim=2))\n",
    "xor.add(Activation(\"tanh\"))\n",
    "xor.add(Dense(2))\n",
    "xor.add(Activation(\"sigmoid\"))\n",
    "\n",
    "xor.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "# Uncomment this line to print the model architecture\n",
    "# xor.summary()\n",
    "\n",
    "# Fitting the model\n",
    "history = xor.fit(X, y, nb_epoch=1000, verbose=0)\n",
    "\n",
    "# Scoring the model\n",
    "score = xor.evaluate(X, y)\n",
    "print(\"\\nAccuracy: \", score[-1])\n",
    "\n",
    "# Checking the predictions\n",
    "print(\"\\nPredictions:\")\n",
    "print(xor.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD\n",
    "This is Stochastic Gradient Descent. It uses the following parameters:<br>\n",
    "\n",
    "Learning rate.\n",
    "Momentum (This takes the weighted average of the previous steps, in order to get a bit of momentum and go over bumps, as a way to not get stuck in local minima).\n",
    "Nesterov Momentum (This slows down the gradient when it's close to the solution).<br>\n",
    "\n",
    "#### Adam\n",
    "Adam (Adaptive Moment Estimation) uses a more complicated exponential decay that consists of not just considering the average (first moment), but also the variance (second moment) of the previous steps.<br>\n",
    "\n",
    "#### RMSProp\n",
    "RMSProp (RMS stands for Root Mean Squared Error) decreases the learning rate by dividing it by an exponentially decaying average of squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

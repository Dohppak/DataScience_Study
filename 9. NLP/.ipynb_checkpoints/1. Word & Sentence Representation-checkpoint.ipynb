{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Representation\n",
    "\n",
    "우리가 사용하는 언어는 상당히 Abstract합니다. 언어는 grounding이 없다고 생각하시면 됩니다.\n",
    "- Sentence : 다양한 길이를 가지는 토큰들의 Sequence입니다.\n",
    "- Token : Vocabulary에 들어가는 요소입니다.\n",
    "    - 형태소 단위\n",
    "    - 어절 단위\n",
    "    - 비트 숫자\n",
    "    - 공백 단위\n",
    "\n",
    "#### Encoding : 컴퓨터가 어떻게 Token을 인식하게 할까?\n",
    "\n",
    "컴퓨터에게 단어를 숫자로 표현하기 위해서, 중복되지 않는 단어장을 만들고, 중복이 없는 index를 부여합니다. 하지만 관계없는 숫자로 인코딩하는것을 원하지 않습니다. 우리는 단어간의 관계를 만들고자 합니다.\n",
    "\n",
    "- Onehot-Encoding : 모든 단어는 동등하게 독립적이다.\n",
    "    - 총 길이가 결정된 단어장 벡터에서, 단어의 index에 위치하는 값은 1, 아닌 나머지는 0으로 구성합니다.\n",
    "    - 모든 토큰간의 거리가 같습니다. 하지만 이 말은 모든 단어간의 의미공간에서 각 단어가 동등하다는 의미를 가집니다.\n",
    "\n",
    "- Word Embedding\n",
    "    - 각 토큰을 연속 벡터 공간에 투영하는 방법입니다. \n",
    "    - Look-up Table : 각 one hot encoding 된 토큰에게 백터를 부여하는 과정입니다. 실직적으로는 one hot encoding 벡터에 벡터 공간을 내적하는 것입니다.\n",
    "    - Word embedding 은 단어의 의미론적 공간입니다. 효율적으로 단어의 상대적인 의미공간을 만들어 낼 수 있습니다.\n",
    "    - 이러한 embedding을 이용하여, POS 태깅, parse trees등을 만들어 낼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11256ffd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {\"hello\":0, \"world\":1}\n",
    "embed = nn.Embedding(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embed(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Representation\n",
    "내가 푸는 문제에 대해서 가장 적합한 representation이 무엇인가?\n",
    "\n",
    "- Continuous Bag-of-word\n",
    "    - 단어장을 단어 주머니로 보게되고, 이에 따라 단어의 순서를 무시합니다.\n",
    "    - 문장에 대한 표현은 단어 백터들을 평균시킨 벡터로 구합니다.\n",
    "        - 하나의 operator node로 문장이 표현이 됩니다.\n",
    "    - Generalize to bag of n-gram\n",
    "        - Token의 N-gram을 적용하기도 편합니다.\n",
    "    - Baseline 모델로 많이 사용이 됩니다.\n",
    "    - 공간에서 가까우면 비슷한 의미, 아니면 다른 의미공간입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Relation Network(Skip-Bigram)\n",
    "    - 문장이 주어졌을 때, 모든 단어들의 pair를 생각합니다. Pair의 representation을 찾게 됩니다.\n",
    "    - 모든 관계를 고려하면서 복잡하게 딥러닝을 보게 됩니다. pair-word representation을 합니다.\n",
    "    $$\n",
    "    h_{t} = f(x_{t},x_{1}) + ... f(x_{t},x_{t-1})+ ... f(x_{t},x_{T})\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional Networks in NLP\n",
    "    - 로컬한 Structure가 중요하다면, 좁은 지역간의 단어의 관계를 잘 표현합니다.\n",
    "    - K-gram을 계층적으로 불 수 있게 됩니다.\n",
    "    - Convolution은 tokens - Multi-word - Phrases - Sentence를 볼수 있게 해준다.\n",
    "    - 1D Convolution Network입니다.\n",
    "    - 긴거리의 Dependency가 있다면 문제가 생깁니다. 특정한 window기준으로 보기 때문입니다.\n",
    "    $$\n",
    "    h_{t} = f(x_{t},x_{t-k}) + ... f(x_{t},x_{t})+ ... f(x_{t},x_{t+k})\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self-Attention\n",
    "    - CNN은 너무 local하게 보는것이고, Relation Network는 너무 global하게 보는것 아닌가?\n",
    "    - CNN이 만약 가중치가 부여된 RN으로 본다면? window는 1, 나머지는 0 매우 hard한 가중치 이지만, 이를 soft하게 만든다면?\n",
    "    $$\n",
    "    h_{t} = \\sum_{t`=1}^{T} \\alpha(x_{t},x_{t'}) f(x_{t},x_{t'}) \\\\\n",
    "    \\alpha(x_{t},x_{t'}) = \\frac {exp(\\beta(x_{t},x_{t'}))}{\\sum_{t`=1}^{T} exp(\\beta(x_{t},x_{t'}))}\n",
    "    $$\n",
    "    - Long range & Short range dependency를 극복가능합니다.\n",
    "    - 관계가 낮은 토큰은 억제하고, 관계가 높은 토큰은 강조 가능합니다.\n",
    "    - 계산 복잡도가 높고 counting 같은 특정 연산이 어렵습니다. $O(T^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN\n",
    "    - RNN은 모든 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

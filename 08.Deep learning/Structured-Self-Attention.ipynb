{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch, keras\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,keras\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    " \n",
    "class StructuredSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
    "    and without pruning. Slight modifications have been done for speedup\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type=0,n_classes = 1):\n",
    "        \"\"\"\n",
    "        Initializes parameters suggested in paper\n",
    " \n",
    "        Args:\n",
    "            batch_size  : {int} batch_size used for training\n",
    "            lstm_hid_dim: {int} hidden dimension for lstm\n",
    "            d_a         : {int} hidden dimension for the dense layer\n",
    "            r           : {int} attention-hops or attention heads\n",
    "            max_len     : {int} number of lstm timesteps\n",
    "            emb_dim     : {int} embeddings dimension\n",
    "            vocab_size  : {int} size of the vocabulary\n",
    "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
    "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
    "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
    "            n_classes   : {int} number of classes\n",
    " \n",
    "        Returns:\n",
    "            self\n",
    " \n",
    "        Raises:\n",
    "            Exception\n",
    "        \"\"\"\n",
    "        super(StructuredSelfAttention,self).__init__()\n",
    "       \n",
    "        self.embeddings,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
    "        self.linear_first = torch.nn.Linear(lstm_hid_dim,d_a)\n",
    "        self.linear_first.bias.data.fill_(0)\n",
    "        self.linear_second = torch.nn.Linear(d_a,r)\n",
    "        self.linear_second.bias.data.fill_(0)\n",
    "        self.n_classes = n_classes\n",
    "        self.linear_final = torch.nn.Linear(lstm_hid_dim,self.n_classes)\n",
    "        self.batch_size = batch_size       \n",
    "        self.max_len = max_len\n",
    "        self.lstm_hid_dim = lstm_hid_dim\n",
    "        self.hidden_state = self.init_hidden()\n",
    "        self.r = r\n",
    "        self.type = type\n",
    "                 \n",
    "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
    "        \"\"\"Load the embeddings based on flag\"\"\"\n",
    "       \n",
    "        if use_pretrained_embeddings is True and embeddings is None:\n",
    "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
    "           \n",
    "        if not use_pretrained_embeddings and vocab_size is None:\n",
    "            raise Exception(\"Vocab size cannot be empty\")\n",
    "   \n",
    "        if not use_pretrained_embeddings:\n",
    "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
    "            \n",
    "        elif use_pretrained_embeddings:\n",
    "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
    "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
    "            emb_dim = embeddings.size(1)\n",
    "            \n",
    "        return word_embeddings,emb_dim\n",
    "       \n",
    "        \n",
    "    def softmax(self,input, axis=1):\n",
    "        \"\"\"\n",
    "        Softmax applied to axis=n\n",
    " \n",
    "        Args:\n",
    "           input: {Tensor,Variable} input on which softmax is to be applied\n",
    "           axis : {int} axis on which softmax is to be applied\n",
    " \n",
    "        Returns:\n",
    "            softmaxed tensors\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    " \n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size)-1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = F.softmax(input_2d)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
    "       \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
    "       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeddings = self.embeddings(x)       \n",
    "        outputs, self.hidden_state = self.lstm(embeddings.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n",
    "        x = F.tanh(self.linear_first(outputs))       \n",
    "        x = self.linear_second(x)       \n",
    "        x = self.softmax(x,1)       \n",
    "        attention = x.transpose(1,2)       \n",
    "        sentence_embeddings = attention@outputs       \n",
    "        avg_sentence_embeddings = torch.sum(sentence_embeddings,1)/self.r\n",
    "       \n",
    "        if not bool(self.type):\n",
    "            output = F.sigmoid(self.linear_final(avg_sentence_embeddings))\n",
    "           \n",
    "            return output,attention\n",
    "        else:\n",
    "            return F.log_softmax(self.linear_final(avg_sentence_embeddings)),attention\n",
    "       \n",
    "\t   \n",
    "\t#Regularization\n",
    "    def l2_matrix_norm(self,m):\n",
    "        \"\"\"\n",
    "        Frobenius norm calculation\n",
    " \n",
    "        Args:\n",
    "           m: {Variable} ||AAT - I||\n",
    " \n",
    "        Returns:\n",
    "            regularized value\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM=3\n",
    "NUM_WORDS= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = train_set[0],train_set[1]\n",
    "x_test,y_test = test_set[0],test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn 34701\n"
     ]
    }
   ],
   "source": [
    "for k,v in word_to_id.items():\n",
    "    print(k,v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([x_train, x_test])\n",
    "y = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = x.shape[0] - 1000\n",
    "n_valid = 1000\n",
    "x_train = x[:n_train]\n",
    "y_train = y[:n_train]\n",
    "x_test = x[n_train:n_train+n_valid]\n",
    "y_test = y[n_train:n_train+n_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train,maxlen=max_len)\n",
    "x_test_pad = pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(torch.from_numpy(x_train_pad).type(torch.LongTensor),torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
    "train_loader = data_utils.DataLoader(train_data,batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 0., 0., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

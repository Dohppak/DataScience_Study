{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting과 Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting과 underfitting은 모형에 Just-right하지않은 모형의 복잡도 혹은 모형의 학습횟수에 따라서 결정이 된다.\n",
    "\n",
    "\n",
    "### 2 Case of Overfitting\n",
    "\n",
    "- 애초에 모형이 너무 복잡하게 설계 되었기 떄문에 발생한 것이다? -> Machine Learning, Deep Learning\n",
    "- Epoch가 증가에 따라서 판별함수가 너무 training set에 fit해졌다? -> Deep Learning\n",
    "- hideen neruons 과 layer에 수에 따라서 복잡해 지는것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity?\n",
    "\n",
    "- 각 뉴런은 layer-by-layer 그리고 feed-forward 관계로 directed acyclic graph 형태입니다.\n",
    "- 각 뉴런들의 activation 과 output은 non-linear transform of weighted sum of inputs 입니다.\n",
    "- Model complexity는 데이터의 feautures 개수에 따라 결정이 됩니다.\n",
    "    - 파라미터 layer의 갯수 or Parameter의 갯수는 영향을 끼치지 않는가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch?\n",
    "\n",
    "- Epoch\n",
    "한번의 forward pass와 한번의 backward pass가 모든 training examples에 대해서 진행되었다는 소리입니다.\n",
    "\n",
    "- batch size\n",
    "한번의 forward pass와 한번의 backward pass 사이에 사용된 data의 갯수를 사용합니다. 만약 batch size가 크다는 소리는, 더 많은 memory space가 필요하다는 소리입니다.\n",
    "\n",
    "- number of Iteration\n",
    "흔히 passes 숫자라고도 합니다. batchsize가 몇번 forward 와 backward를 반복했는지를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we detact Over & Underfitting\n",
    "\n",
    "전체 데이터 셋을 Test, Train, Validation set으로 나누어서 cross validation을 진행한다.\n",
    "\n",
    "- 3개의 데이터셋 분포를 파악해야한다. Sampling에 따른 모집단의 추종여부를 확인할 것!\n",
    "- High Bias : Underfitting, 모델의 퍼포먼스가 좋지않다.\n",
    "    - Sol : 모델의 복잡도를 높이거나 Epoch를 증가시킨다.\n",
    "- High Variance : Overfitting, Train set에 fit하다.\n",
    "    - Overfitting이 발생하는 곳에서 Early stopping을 한다.\n",
    "    - Sol : 데이터의 갯수를 늘리거나, Regulariazation을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![of1](img/06.of1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch 증가의 경우\n",
    "\n",
    "- Underfitting : Training Error, Test Error의 차이가 거의 적고, Error의 크기가 매우 크다. (High Bias)\n",
    "- Just right : Training error 는 작고, test error 역시 작다 매우 이상적인 상태이다. \n",
    "- Overfitting : Training error는 작고, test error 는 큰 상태이다. (High Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![of2](img/06.of2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about Error\n",
    "\n",
    "- Noise : 관측에서 오는 에러 (통제 불가), 확률프로세스이거나, 관측이 안된 무언가가 있거나\n",
    "- Bias : 타겟 함수($y$) - 추정함수($\\hat{y}$)의 평균의 차이이다.\n",
    "- Variance : 추정 평균과 추정값의 차이이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Weight & Overfitting\n",
    "\n",
    "- Large Weight 가 overfitting을 만든다.\n",
    "- Regularization을 해주자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/06.RG.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we solve Over & Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regluarlization\n",
    "\n",
    "Regularization은 일반화의 성능을 올려주는 방법입니다. 이는 우리가 일반적으로 Overfitting이 감지 되었을때 사용하는 방법입니다. 뉴럴넷의 weight값을 0에 가깝게 만들어주는 효과가 있습니다.\n",
    "\n",
    "$\\lambda$ 가 커지면 Activate function에 들어가는 input값이 0에 가까워지게 됩니다. 그러면서 activation함수의 linear한 영역을 통과하게 됩니다.\n",
    "\n",
    "- L2 norm\n",
    "    - 모든 Parameters의 영향력을 정규화합니다.\n",
    "    - 뉴럴넷에서는 일반적으로 Frobenius norm이라고 부릅니다.\n",
    "    - Weight decay의 효과도 있습니다.\n",
    "    \n",
    "- L1 norm\n",
    "    - 특정 Parameters의 영향력을 정규화합니다. (sparse solutions이 됩니다.)\n",
    "    - 모델을 압축하기 위한 목적이 없다면 딱히 사용하진 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "더 작은 뉴럴넷 모형을 상용해서, 정규화 효과를 줍니다. 또한 Dropout으로 인해서 각 노드들은 이제 어떠한 feature에 의존하는 경향으 줄어들게 됩니다. 때문에 가중치를 줄이게 되는 효과를 줍니다. 각 레이어들 마다 keep_prob 의 값을 바꾸어줄수 있다.\n",
    "\n",
    "feedforward pass에서, hidden layer unit들을 랜덤하게 끊어버린다. 이는 hidden unit들의 co-adaptiation을 방지한다. 이던 feature의 reduendency를 만들며, waste of resource가 발생한다.\n",
    "- Co-adaptation : 두개나 더 많은 hidden이 같은 feature를 학습하는 것\n",
    "\n",
    "또한 Dropout은 큰 ensemble 모델처럼 사용이 가능하다. 앙상블 모형은 모델은 variance를 줄여준다. Fully connected layer의 1024 유닛은 2^1024은 masks combination을 만든다. 그들은 파라미터를 공유한다.\n",
    "\n",
    "- Inverted dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "problem은 zero mean과 unit variance에 있다.\n",
    "- input normalization\n",
    "    - Standadization : 평균으로 빼고, std로 나누어줍니다. 문제는 Std가 0이 되면 큰일납니다.\n",
    "    - PCA : orthogonal 한 latent axis로 projection 시켜줍니다. 일반적으로 뉴럴넷에서 학습해주기 때문에, reduendency를 만드는 경향이 있습니다.\n",
    "- Weight Initalization\n",
    "    - Gaussian random number\n",
    "        - 깁은 뉴럴넷 레이어의 경우에는, 문제가 생기는 경우가 많습니다.\n",
    "    - Xavier initalization\n",
    "        - hanh function에 잘 작동합니다.\n",
    "    - He initalization\n",
    "        - ReLU에 잘 됩니다.\n",
    "\n",
    "- Batch normalization\n",
    "    - Gradient Vanishing, Gradient Exploding 문제\n",
    "    - Internal covariance shift? \n",
    "    - 새로운 layer를 FC, Conv 와 non-linear function 사이에 넣어줍니다.\n",
    "    - activation function의 linear 구간을 scaling 해주고 shift 해주면서 더 좋은 nonlinearity position을 잡아냅니다.\n",
    "    \n",
    "- Layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "데이터의 variance를 주면서 데이터의 양을 증가시킵니다. 오디오 데이터의 경우에는 다음과 같은 방식이 있습니다.\n",
    "- Pitch Shifting\n",
    "- Resampling\n",
    "- Time-stretching\n",
    "- Equalization\n",
    "- Adding noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

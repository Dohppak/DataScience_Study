{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architectures\n",
    "\n",
    "이미지 분류에 있어서는 다양한 모형들이 ImageNet Challenge 를 통해서 나오게 되었습니다.\n",
    "\n",
    "### AlexNet (2012)\n",
    "\n",
    "Krizhevsky et al. 힌튼박사님의 박사과정 학생이였습니다.\n",
    "\n",
    "- 2개의 평행한 Convolution flow를 설계하게 됩니다. 2개의 GPU를 사용했기 때문이라고 합니다.\n",
    "- 224x224x3 의 input\n",
    "- 첫 conv는 11x11사이즈의 96개의 filter를 적용했습니다. \n",
    "- 두번째 conv는 5x5\n",
    "- 최초로 ReLU를 사용\n",
    "- Norm layers : Local Response Normalization (LRN)\n",
    "    - 활성화를 넘어서 2번째 레이어로 넘어간 부분에 대해서 Local적인 부분에 Normalization을 걸어주게 됩니다.\n",
    "    - 오늘날에는 batch normalization을 하게됩니다.\n",
    "- Heavy data augmentation\n",
    "- Dropout : 0.5\n",
    "- Batch Size : 128\n",
    "- SGD Momentum : 0.9\n",
    "- Learning rate with 1e-2 with anneling\n",
    "- L2 weight decay : 5e-4\n",
    "- 7CNN ensemble\n",
    "\n",
    "### VGGNet (2014)\n",
    "\n",
    "Simonyan and Zisserman\n",
    "\n",
    "- Small filter(3x3) and deeper network\n",
    "- Current standard deep CNN\n",
    "- Stack of two 3x3 filter\n",
    "    - Why? 3x3 filter를 두개 넣으면 5x5보다 파라미터가 작기 때문입니다.\n",
    "- NO LRN\n",
    "- ensembles\n",
    "- Total Memory : 24M * 4 bytes ~= 96MB / image\n",
    "- Total Paramas : 138M parameters\n",
    "\n",
    "### GoogLeNet (2014)\n",
    "\n",
    "Szegedy et al.\n",
    "\n",
    "- Deeper network with computional efficiency\n",
    "- No FC layer\n",
    "- Inception module\n",
    "    - Parallel filter multiple receptive field sizes\n",
    "    - Reduce parameters using 1x1 conv?\n",
    "        - 1x1 conv가 무엇일까요? 1x1 filter의 depth가 64개로 해서 depth를 줄이는 효과를 주게 됩니다.\n",
    "    - Only 5M parameters : x12 less than VGGNet\n",
    "    \n",
    "\n",
    "### ResNet (2015)\n",
    "\n",
    "- Very deep netwrok is hard to train!\n",
    "    - 레이어를 쌓을 수록, test error가 떨어지지 않습니다. \n",
    "- Residual connections 과 Highway를 넣어줍시다.\n",
    "    - Convolution 마다 Skip-connection을 뚫어주는 것이 아이디어 입니다.\n",
    "    - 이때 Same convlution을 통해서 같은 차원을 유지해야합니다\n",
    "    - 차원이 다를시에는 $a^{[l]}$에 linear transform을 해줄 wight를 곱해주어야합니다.\n",
    "    - relu에 의해 항등함수를 학습하기 매우 쉽습니다, 수행능력이 떨어지지 않는다는 것을 보장할 수 있습니다.\n",
    "- Loss function이 훨신 낮아보이게 됩니다.\n",
    "\n",
    "$$\n",
    "a^{[l+2]} = g(z^{[l+2]}+a^{[l]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network in network\n",
    "\n",
    "1x1 convolution을 사용합니다. 일반적으로 Channel이 너무 클때, 이 채널의 지역적인 정보를 압축하고 싶을때 사용을 하게 됩니다. 또한 1x1를 쓰는것은 non-linearity를 주는 효과도 줄 수 있습니다. \n",
    "\n",
    "- 일반적으로 Channel이 1 이상인 경우 매우 중요한 역할을 합니다.\n",
    "- 채널간의 지역적인 특성을 반영할 수 있다는 장점을 가지게 됩니다. \n",
    "\n",
    "### Inception network\n",
    "\n",
    "1x1, 3x3, 5x5, pooling중에 어떤것이 좋은 컨볼루션일까요? Inception 모듈의 가장 큰 특징은 바로 이것입니다. 28x28x192짜리 input데이터를 다양한 Convolution과 pooling layer를 사용하여 28x28x256짜리 데이터로 변환시킵니다. \n",
    "\n",
    "하지만 이 계산은 높은 연산량을 시키게 됩니다. 만약 28x28x192를 5x5x32 Same Conv를 쓰게 된다면, 120M의 연산량이 발생하게 됩니다. 이러한 높은 연산량을 줄이기 위한 방법이 바로 위에서 소개한 1x1 Conv입니다.(bottleneck layer)\n",
    "\n",
    "- 28x28x192\n",
    "- Conv 1x1x192가 16개 존재하게 됩니다\n",
    "- 28x28x16\n",
    "- Conv 5x5x16이 32개를 배치합니다\n",
    "- 28x28x32\n",
    "\n",
    "120M에서 12.4M까지 연산량이 줄어들게 됩니다.\n",
    "\n",
    "- Side branch를 사용하여 중간중간 output 예측을 하게 만들어 줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Weight\n",
    "\n",
    "First Layer\n",
    "- Simply visualizae each column of the matrix\n",
    "    - Acoustic-level features : harmonic/percussive/how high energy\n",
    "- Find songs that maximize the filter activations\n",
    "    - The genres of the tracks in these playlists are quite different\n",
    "\n",
    "Upper Layer\n",
    "- Find songs that maximize the filter activation\n",
    "- The Genres of the tracks become more similar in the playlists\n",
    "- How can we visualizae the features in upper layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconvolutional Network\n",
    "\n",
    "데이터셋에서 Convolution을 거쳐서 feature Map을 하고 feedforward로 진행시킨다.\n",
    "\n",
    "### DeconvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet Implementation\n",
    "\n",
    "ResNet의 핵심모듈은 shortcut 혹은 skip connection입니다. 이는 2개의 큰 block으로 구성되게 됩니다. 첫번째는 identity blcok과 Convolution blcok입니다.\n",
    "\n",
    "__The identity block__\n",
    "\n",
    "Identity block은 Resnet의 기본적인 block이 됩니다. 일반적으로 우리가 구성하는 conv layer는 Convolution layer와 batch norm layer 그리고 ReLU로 구성됩니다. 하지만 Resnet의 Identity block은 input activation($a^{[l]}$)을 2레이어 뒤의 같은 차원의 output activation에 더해주는 것입니다. \n",
    "\n",
    "__Arguments__\n",
    "\n",
    "X : 2차원의 이미지 데이터입니다. (Batch, n_H_prev, n_W_prev, n_C_prev)\n",
    "f : integer, specifying the shape of the middle CONV's window for the main path\n",
    "filters : python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "stage -- integer, used to name the layers, depending on their position in the network\n",
    "block -- string/character, used to name the layers, depending on their position in the network\n",
    "\n",
    "Returns:\n",
    "X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1,dilation=1):\n",
    "    return nn.Conv2d(in_planes,out_planes,kernel_size=3, stride=stride, \n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes,out_planes,kernel_size=1, stride=stride,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1,downsample=None, groups=1,\n",
    "                base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups = 1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        \n",
    "        # conv1 과 downsample layer는 샘플들의 정보를 점점 압축시킵니다.\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(plane *(base_width/64.)) * groups\n",
    "        # self.conv2 와 self.downsample layer는 downsample역할을 하게 된다.\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplanes=True)\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "딥러닝이란 무엇일까요? 일반적으로는 End to end feature learning 으로 Domain knowledge에 대한 의존성이 낮으면서 해결 할 수 있습니다.\n",
    "\n",
    "- Traditional Pattern Recognition\n",
    "    - 여러가지 Fixed 되어있고 Handcrafted 한 Feature Extreaction 기법을 통해서 Feature를 추출하고 학습을 시켰습니다. 이 당시에는 main knowledge가 매우 중요했습니다.\n",
    "- Mainstram Modern pattern Recognition\n",
    "    - Feature Extraction 이후에 Unsupervised learning을 통해 mid-level feature가 뽑혀서 나오기 시작했습니다.\n",
    "- Representation are hierarchical and trained\n",
    "    - Deep learning의 시대가 되었습니다. 각 layer는 feature를 learning 시키는 역할을 하게 되었습니다. 이는 인간이 feature를 이해하는 학습 시스템과 점점 더 비슷하게 변하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 모델 Building\n",
    "    - Connectivity patterns\n",
    "    - Nonlinearity Modules\n",
    "    - Loss function\n",
    "- 모델 학습\n",
    "    - Optimization\n",
    "    - Hyper Parameters\n",
    "\n",
    "### Connectivity Pattern\n",
    "\n",
    "딥러닝은 여러 레이어를 쌓아나가는 구조입니다. 뉴런은 각 레이어에 있으며, 레이어들간의 연결관계에 따라서 패턴이 나눠집니다.\n",
    "- Fully-Conntected\n",
    "- Convolutional\n",
    "- Dilated\n",
    "- Recurrent\n",
    "- Skip / Residual\n",
    "- Random\n",
    "\n",
    "### Nonlinearity modules\n",
    "\n",
    "왜 Nonlinear function을 Hidden layer에 쓰는가? 이유는 input units들의 interaction을 확인하기 위해서 입니다. 이 non-linear function으로 인해서 \n",
    "\n",
    "만약 linear Mutiplication만 진행한다면, 결국 layer를 여러개 쌓을 이유가 없습니다. Multiplication of linear transformation = another linear transformations. \n",
    "\n",
    "hidden layer는 어떠한 feature라고 생각할수 있다. 그렇다면 hidden layer space에서 본다면, \n",
    "- ReLU\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- GRU\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss\n",
    "손실함수를 정의하는 방식입니다. 학습의 핵심을 만들어주는 기능도 같이 해주는 경향이 있습니다.\n",
    "\n",
    "- Cross Entropy\n",
    "- Adversarial\n",
    "- Variational\n",
    "- MLE\n",
    "- L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "학습의 진행을 도와줍니다.\n",
    "\n",
    "- SGD\n",
    "- Momentum\n",
    "- RMSProp\n",
    "- Adagrad\n",
    "- Adam\n",
    "- Second Order\n",
    "\n",
    "### Hyper Parameter\n",
    "- Learning rate\n",
    "- Weight decay\n",
    "- Layer size\n",
    "- Batch size\n",
    "- Dropout rate\n",
    "- Weight initalization\n",
    "- Data augmentation\n",
    "- Gradient Clipping\n",
    "- Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

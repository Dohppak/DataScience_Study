{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "각 class를 분류하기 위해서, 이제 모든 클래스는 자신만의 decision boundary를 만들게 됩니다. \n",
    "\n",
    "- SoftMax function : The equivalent of the sigmoid activation function, But when 3 or more classes\n",
    "- 소프트멕스 함수 : 다양한 class 에 대해서 sigmoid와 같은 활성화 함수 기능을 합니다.\n",
    "    - Expoential function : 오직 positive values 만 returns 합니다.\n",
    "    - 비 선형적이며, 실수 벡터를 취하여 확률분포로 반환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(L):\n",
    "    # 지수함수로 보낸다.\n",
    "    expL = np.exp(L)\n",
    "    # 지수함수에 있는 애들을 합친다.\n",
    "    sumExpL = sum(expL)\n",
    "    # list 자료형\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax2(L):\n",
    "    expL = np.exp(L)\n",
    "    return np.divide(expL, expL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax2([5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113da8230>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2847, 0.1919, 0.1563, 0.2735, 0.0935])\n",
      "tensor(1.)\n",
      "tensor([-1.2563, -1.6507, -1.8559, -1.2963, -2.3695])\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())\n",
    "print(F.log_softmax(data, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "- Input data 가 categorical data 라면?  1,0 으로 bool 값을 표현하는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "각 데이터의 label 의 확률을 계산하여 $P(all)$값이 얼마나 모델이 정확한지를 나타나게 만든다!\n",
    "- Maximize probability and Minimize error function 을 같은 의미를 가지게 만들어 주자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy (Connect probabilities and error functions)\n",
    "\n",
    "- logarithm : 로그함수는 곱셈을 덧셈으로 바꾸는 성질이 있다. (summation 만 하면 충분하다!), 여기에 negative를 주자!\n",
    "- 나쁜 모델에게 높은 Cross Entropy를 좋은 모델에게는 낮은 Cross Entropy를 부여하게 한다.\n",
    "- Cross-entropy가 높다는 의미는 낮은 확률로 event가 발생한다인 것이다.\n",
    "- 정확도가 높은것은 확률이 1에 가까워진다. 그 말은 정확도가 높을 수록 1에 가까워지며 이는 log에 들어가면 0에 가까워진다. \n",
    "\n",
    "#### 정확도가 높은 확률의 최대화가 Cross-Entropy의 최소화와 비슷한 의미를 가지게 만들 수 있다.\n",
    "\n",
    "__Negative log-likelihood__\n",
    "\n",
    "주어진 데이터를 통해 미지의 최적 모델 파라미터 $\\theta$를 찾는 방식입니다. 주어진 데이터는 일반적으로 $X,Y$ 가 있는 pair입니다. 하지만 우리가 일반적으로 활용하고 싶은 분야는 입력$X$와 모델 파라미터 $\\theta$가 주어졌을 때, 정답 $Y$가 나타날 확률입니다. $P(Y|X;\\theta)$를 최대화 하는 $\\theta$를 찾는 것이라고 보면 됩니다. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\theta_{ML} &= \\text{argmax}_{\\theta} P_{model}(X|\\theta) \\\\\n",
    "&= \\text{argmax}_{\\theta} {E_{X \\sim P}}[\\log P_{model}(x| \\theta)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "__Cross Entropy__\n",
    "\n",
    "크로스 엔트로피의 최소화는 우도의 최대화와 본질적으로 비슷합니다. 만약 데이터의 분포를 $P(x)$와 모델이 추정한 데이터의 분포를 $Q(x)$라고 할때, 우리는 둘의 차이를 KL Divergence를 통해서 구할 수 있습니다.\n",
    "$$\n",
    "D_{KL}(P||Q) = E_{X\\sim P}\\left[\\log \\frac{P(x)}{Q(X)}\\right] = E_{X\\sim P}\\left[ \\log P(x) - \\log Q(x) \\right]\n",
    "$$\n",
    "$P$와 $Q$가 동일한 확률분포라면 그 값은 0이 됩니다.\n",
    "\n",
    "KLD는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.828313737302301, 4.8283137373023015)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy([1,0,1,1],[0.4,0.6,0.1,0.5]),cross_entropy([1,1,0,1],[0.4,0.4,0.9,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClass Cross-Entropy \n",
    "\n",
    "- Y-label의 class가 2개가 아니라 그 이상일때는 어떻게 해야할까?\n",
    "각 class별로 Summation 하고 평균내자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

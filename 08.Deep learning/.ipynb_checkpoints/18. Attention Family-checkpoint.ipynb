{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Family of Attention Mechanism\n",
    "\n",
    "Attention은 Alignment score function을 무엇으로 할것인가 그리고 input과 target 과의 관계에 따라 self 인지 아닌지, Attention의 범위에 따라 Global과 Soft, 그리고 방식에 따라 Local과 Hard로 나누어지게 됩니다. 우리는 여기서 Self-Attention에 대해서 알아보려고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Name | Alignment score function | Citation |\n",
    "| -------------------------- | ------------- | ------------- |\n",
    "| Content-base attention | $$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\text{cosine}[\\boldsymbol{s}_t, \\boldsymbol{h}_i]$$ | [Graves2014](https://arxiv.org/abs/1410.5401) |\n",
    "| Additive(*) | $$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_t; \\boldsymbol{h}_i])$$ | [Bahdanau2015](https://arxiv.org/pdf/1409.0473.pdf) |\n",
    "| Location-Base | $$\\alpha_{t,i} = \\text{softmax}(\\mathbf{W}_a \\boldsymbol{s}_t)$$<br/>Note: This simplifies the softmax alignment to only depend on the target position. | [Luong2015](https://arxiv.org/pdf/1508.04025.pdf) |\n",
    "| General | $$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top\\mathbf{W}_a\\boldsymbol{h}_i$$<br/>where $\\mathbf{W}_a$ is a trainable weight matrix in the attention layer. | [Luong2015](https://arxiv.org/pdf/1508.04025.pdf) |\n",
    "| Dot-Product | $$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top\\boldsymbol{h}_i$$ | [Luong2015](https://arxiv.org/pdf/1508.4025.pdf) |\n",
    "| Scaled Dot-Product(^) | $$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\frac{\\boldsymbol{s}_t^\\top\\boldsymbol{h}_i}{\\sqrt{n}}$$<br/>Note: very similar to the dot-product attention except for a scaling factor; where n is the dimension of the source hidden state. | [Vaswani2017](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) |\n",
    "\n",
    "(\\*) 은 concat을 의미합니다 Luong, et al., 2015 그리고 \"additive attention\" 역시 동일합니다. in Vaswani, et al., 2017.\n",
    "\n",
    "(^) 은 scaling factor입니다. $1/\\sqrt{n}$, 인풋이 매우 클때, softmax function이 극도록 작아지는 gradient를 만들어 학습이 어려워 지는 현상을 보안했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Definition | Citation |\n",
    "| -------------------------- | ------------- | ------------- |\n",
    "| Self-Attention(&) | Relating different positions of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace the target sequence with the same input sequence. | [Cheng2016](https://arxiv.org/pdf/1601.06733.pdf) |\n",
    "| Global/Soft | Attending to the entire input state space. | [Xu2015](http://proceedings.mlr.press/v37/xuc15.pdf) |\n",
    "| Local/Hard | Attending to the part of input state space; i.e. a patch of the input image. | [Xu2015](http://proceedings.mlr.press/v37/xuc15.pdf); [Luong2015](https://arxiv.org/pdf/1508.04025.pdf) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

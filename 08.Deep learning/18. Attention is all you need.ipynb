{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "2017년에 Vaswani, et al이 낸 이 논문은 2019년도 기준 3695회 인용되어 학계에 엄청난 영향력을 끼치고 있는 논문입니다. 주된 Propsed Model은 __Transformer__ 로 seq2seq모델을 self-attention mechanisms을 활용하여 recurrent network units을 없이 구현한 것에 있습니다. 이는 가장 성공적으로 Attention Mechanism을 encoder와 decoder사이에 넣어주었다는 점입니다. \n",
    "\n",
    "### Main Contribution\n",
    "__Paralleization of Seq2Seq__<br>\n",
    "RNN과 CNN을 활용한 Seq2Seq모델이 많이 나오기 시작했습니다. 하지만 RNN의 경우에는 Parallelization이 안되는 문제점과, Long-range dependencies 가 발생한다는 문제점이 있었습니다. 이러한 문제점은 Sequence-aligned states를 구현하기 어렵다는 점과, Hierarchical domain에서 힘을 쓸수 없다는 문제점이 나오게 됩니다. 그렇다면 CNN의 경우는 어떨까요? CNN은 Paralleize를 layer별로 구현이 가능합니다. 하지만 input sequence의 증가에 따라서 CNN의 사이즈가 커지게 되거나, padding size가 증가하여 연산의 비효율을 가져오게 됩니다. 실제로도 ConvC2C $O(n)$, ByteNet $O(nlogn)$의 연산량을 통해서 Distant position에 대한 학습의 어려움을 보였습니다.\n",
    "\n",
    "__Reduce sequential computation__<br>\n",
    "Constant O(1) number of operations to learn dependency between two symbols independently of their position distance in sequence.\n",
    "\n",
    "### Key, Value, and Query\n",
    "Transformer의 가장 중요한 유닛은 바로 Multi-head self-attention mechanism입니다. 우선이 Transformer는 Input을 encoded representation인 __Key-Value Pair__ $(K,V)$로 바라보게 됩니다. 이 둘은 모두 n 차원의 백터이죠(사실상 input sequence length). NMT Context에서는 이 두가지는 encoder의 hidden states입니다. 그리고 Decoder에서는 Previous ouput이 __Query__ 로 압축되어 들어오게 됩니다. (Q of dimension m) 그리고 다음 ouput은 query와 (key, value) pair의 맵핑으로 생성되게 되는 것입니다. \n",
    "\n",
    "이 Transformer 는 Scaled dot-product attention을 도입하였습니다. \n",
    "$$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{a}_i) = \\frac{\\boldsymbol{s}_t^\\top\\boldsymbol{a}_i}{\\sqrt{n}}$$\n",
    "$$\\text{Attention}(\\boldsymbol{Q,K,V}) = \\text{softmax}(\\frac{\\boldsymbol{K}^\\top\\boldsymbol{Q}}{\\sqrt{n}})\\boldsymbol{V}$$\n",
    "이는 Value의 관점에서 보게 된다면 Wegithed Sum의 형태입니다. 이는 Value를 어디로 Assignment할 것인가에 대한 대답이 Query와 Key의 dot-product로 결정된다는 이야기입니다.\n",
    "\n",
    "### Multi-head Attention\n",
    "Attention을 한번 연산하기 보다는, Multi-head mechanism은 Scaled dot-product attention을 parallel하게 연산하게 해줍니다. \"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different position.\"\n",
    "\n",
    "### Encoder \n",
    "Encoder는 Location 정보를 가지고 있는 attention-based representation을 생성합니다. 이때 정보는 무한하게 큰 context로 부터 생성되게 됩니다. 각각의 Multi-head self-attention layer입니다. 그리고 이것들은 simple position-wsie fully connected feed-forward network로 구성되어 있습니다. 그리고 각각의 Sub-layer는 residual connection을 가지고 layer normalization을 취하게 됩니다. 그리고 모든 sub-layer의 output은 같은 Dimension을 가지게 됩니다.\n",
    "__Positional Encoding__<br>\n",
    "\n",
    "### Decoder\n",
    "Decoder도 Encoder와 비슷합니다. 각각의 Layer는 두개의 sub layer를 가지고 있고, Multi-head attention을 사용하고 마지막에 fully-connected feed forward network를 사용하고 있습니다. 또한 residual connection과 layer normalization 역시 동일합니다. 가장 큰 차이를 보이는 점은 바로 Masked Multi-head attention 파트입니다. 현재의 position에서 미래의 Target sequence를 보지 않기 위해서 입니다. \n",
    "\n",
    "### Full Architecture\n",
    "Transformer의 전체 Architecture는 다음과 같습니다. source와 Target sequence들은 같은 차원을 가지게 됩니다. Location 정보를 유지하기 위해서 정현파를 기반으로 하는 positional embedding이 사용되었습니다. 마지막 레이어에서 linear와 softmax function이 사용되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input sentence, word embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

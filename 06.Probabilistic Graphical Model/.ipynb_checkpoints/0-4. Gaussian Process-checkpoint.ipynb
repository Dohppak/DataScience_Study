{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process\n",
    "\n",
    "A random process $X(t)$ is a Gaussian process if for all $k ∈ N$ for all $t_{1},...,t_{k}$, a random vector formed by $X(1),...,X(t_{k})$ is jointly Gaussian.\n",
    "<br>\n",
    "\n",
    "랜덤 프로세스 $X(t)$가 가우시안 프로세스가 될려면, 모든 시점에 대해서, 시점에 대한 확률변수가 joint Gaussian이면 Gaussian Process라고 한다.\n",
    "<br>\n",
    "\n",
    "The joint density is completely specified by\n",
    "- Mean: m(t) = E(X(t)), where m(·) is known as a mean function.\n",
    "- Covariance: k(t,s) = cov(X(t),X(s)) =, where k(·,·) is known as a covariance function.\n",
    "- Notation: $X(t) ∼ GP(m(t),k(t,s))$\n",
    "\n",
    "Random Variable의 collection 이다. Density는 시간에 대한 평균과, 시간과 공분산이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Gaussian process란 무엇인가? \n",
    "\n",
    "\n",
    "\n",
    " 이번 포스팅의 주 목적인 Gaussian process (GP)에 대해서 생각해보자. GP는 그 이름에서도 알 수 있듯이 RP의 일종이다. 그리고, 그 관계가 몬가 가우시안 적일 것이다. 내가 지금 참고하고 있는 논문 [1]의 노테이션을 사용해서 설명해보겠다. \n",
    "\n",
    " \n",
    "\n",
    " 먼저 RP는 다음과 같이 정의한다. \n",
    "\n",
    "Probability space는 '$(\\Omega, F, P)$'이고, '$\\Omega$'는 sample space이다. 확률 변수의 모음으로 '$Z(x, w)$'로 나타난다. 이 때 '$x$'는 indexing variable이고, '$w$'는 Sample space의 event이다. \n",
    "\n",
    "\n",
    "\n",
    " 이러한 RP의 mean function과 covariance function은 다음과 같이 정의된다. \n",
    "\n",
    "$$ \\mu(x) = E(Z(x, w)) = \\int_{\\Omega} Z(x, w)dP(w) $$\n",
    "\n",
    "$$ C(x_i, x_j) = Cov(Z(x_i, w), Z(x_j, w)) = E((Z(x_i, w) - \\mu(x_i))(Z(x_j, w) - \\mu(x_j))) $$\n",
    "\n",
    "$$ = \\int_{\\Omega} (Z(x_i, w) - \\mu(x_i))(Z(x_j, w) - \\mu(x_j)) dP(w)$$\n",
    "\n",
    "\n",
    "\n",
    " 여기서 부터는 '$Z(x, w)$'에서 '$w$'의 dependency를 생략하겠다. 무슨 의민고 하니, 어떤 sample space로부터 한 event가 검출되고, 해당 event로부터 index '$x$'에 의해 어떤 한 값이 결정되는데, 이제부터는 '$Z$'가 index '$x$'에 soley dependent한다고 보는 것이다. 예를 들어 '$Z$'가 온도를 나타내고, '$x$'가 위치를 나타내면, 온도는 위치에 함수로 보겠다는 의미이다. 그리고 우리는 무한 개가 아닌 다음과 같은 유한 개의 set을 가정할 것이다. \n",
    "\n",
    "$$ \\{ Z(x_1), Z(x_2), ..., Z(x_n) \\} $$ \n",
    "\n",
    "물론 '$Z(x_i)$'는 '$Z(x), x \\in X$'의 collection이다. \n",
    "\n",
    "\n",
    "\n",
    " Gaussian process는 유한개의 finite dimensional distribution이 multivariate normal distribution을 따르는 RP를 의미한다. 다시 말해서 유한개의 확률 변수를 뽑아냈을 때 그것들이 모두 joint Gaussian distribution을 따른다. 그렇기 때문에 GP는 그것의 mean function과 covariance function 만 있으면 정의가 된다. 이는 마치 joint Gaussian distribution이 mean vector와 covariance matrix로 정의됨과 마찬가지이다. 또한 covariance function은 다음의 조건을 만족하는 positive definite function이어야 한다. \n",
    "\n",
    " $$ \\sum_{i=1}^{n}\\sum_{j=1}^{n} a_ia_jC(x_i, x_j) > 0 $$\n",
    "\n",
    "for every '$n$', every collection '$\\{ x_1, x_2, ... , x_n \\}$' and every vector '$a$'.\n",
    "\n",
    "\n",
    "\n",
    " 일반적인 GP는 mean-zero를 가정한다. 그리고 covariance function input 들의 차이(separation vector)의 함수이다. 다시 말해서 위에서 정의한 wide-sense stationary의 조건을 모두 만족한다. 추가로 만약 covariance가 distance metric 의 함수라면 해당 process는 isotropic이라고 한다. 이렇게 정의된 covariance function을 통해서 우리에게 익숙한 kernel (correlation) function을 유도할 수 있다. \n",
    "\n",
    "$$ R(x_i, x_j) = \\frac{C(x_i, x_j)}{\\sigma(x_i)\\sigma(x_j)} $$\n",
    "\n",
    "'$\\sigma^2(x_i) = C(x_i, x_i)$' 는 variance function 이다. \n",
    "\n",
    "\n",
    "\n",
    " wss를 만족하는 Gaussian process를 stationary Gaussian process라 한다. 물론 non-stationary GP도 있을 것이다. 이는 우리의 kernel function이 거리의 함수가 아님을 의미한다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

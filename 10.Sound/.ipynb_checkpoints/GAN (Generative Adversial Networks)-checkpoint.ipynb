{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/YBIGTA/Deep_learning/blob/master/GAN/2017-07-21-First-GAN.markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN (Generative Adversarial Nets)\n",
    "\n",
    "대립되는 두개의 신경망을 동시에 학습시키면서 원본의 Sample과 유사한 Sample을 만드는 방식입니다. 여기서 두개의 역할이 바로 Generator와 Discriminator인데, Generator는 실제 지폐와 똑같이 생긴 지폐를 만드려고 노력하는 위조 지폐범이고 Discriminator는 이것을 구별하려는 경찰입니다.\n",
    "\n",
    "기존의 하나의 cost function을 가지고 이를 최적화 했던 다른 신경망 방식과 다르게,두 개의 신경망을 동시에 학습시키면서 Generator는 Discriminator의 구분확률을 최대한 줄이고, 이와 동시에 Discriminator는 Real Sample과 Fake Sample의 구분 정확도를 높이는데 목적을 두고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특징\n",
    "GAN은 기존 머신러닝 기법과 같이 원데이터의 분포에 대해 직접적으로 알아내고 분석하기보다는 원데이터와 최대한 비슷한 Sample을 만들어 내는데 목표를 둔다는 점입니다. 따라서 원 데이터가 매우 복잡하고 고차원이거나, 원데이터가 무한히 많을 때 좋은 성능을 내게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "Generative Model은 데이터의 분포를 학습하고, 이로부터 새로운 데이터를 생성 가능한 모형입니다.\n",
    "\n",
    "- Generative model에서는 확률 계산의 어려움\n",
    "- Generative model에서 ReLU를 적용하는데 어렵습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Nets\n",
    "무작위로 만들어낸 noise를 Generative model에 전달하고, 이 값이 신경망을 거치면서, 샘플을 만들어내는 것입니다. 이러한 Adversial Nets은 게임이론으로부터 아이디어를 얻게 되는데요. 게임이론에서는 상반된 두 플레이어가 균형점을 찾는 경우를 설명할때가 있습니다. 바로 Adversarial Networks는 이 상반된 플레이어를 Generator와 Discriminator로 설정하고, 그 두가지 네트워크의 상반된 Cost function을 이용하여 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Problem\n",
    "\n",
    "높은 차원에서, 연속적이고 non-convex한 균형점을 찾아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train\n",
    "무작위로 만들어낸 noise를 Generative model에 전달하고, 이 값이 신경망을 거치면서, 샘플을 만들어내는 것이다. Generater는 데이터의 분포를 모방하는 것이 목표이다. 즉 어떤 데이터 $x$가 주어졌을 때 이 데이터 $x$의 분포인 $p_x$를 모방해야한다.\n",
    "\n",
    "먼저 해야하는 것은 noise variable $z$를 정의해야한다. $z$는 어떠한 확률분포 $p_z$를 따르게 될 것이다. 이러한 noise variable을 샘플링하는 것을 $p_z(z)$라고 할수 있다. 이렇게 샘플링한 $z$값을 Generater가 따르는 확률분포 $p_g$로 맵핑할수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generator__<br>\n",
    "Generator의 입장에서 보면 입력값은 $p_z(z)$에서 샘플링된 값이고, 이를 노이즈를 줘서 fake sample을 만들어 Discriminator에게 보내게 됩니다. 후에 Discriminator의 반응에 따라 신경망을 최적화 하는데, 즉 어떤 Sample에 대해서는 1(real), 0(fake)인지에 따라서 Generator가 최적화 됩니다. Generator를 Sample이 실제로 real인지 fake인지 모르고, Discriminator가 어떻게 평가했는지를 보고 최적화 하게 됩니다.\n",
    "\n",
    "__Discriminator__<br>\n",
    "다음으로 Discriminator의 입장에서 말하자면 입력값은 real sample과 fake sample이고 이들이 실제로 real인지 fake인지의 여부입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "일반적으로 GAN의 Generator와 Discriminator의 hidden layer에서는 Leaky ReLU를 사용하게 되는데 그 이유는 Generator는 Discriminator로 부터 Gradient를 받게 되는 구조이기 때문입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
